version: '3'
x-airflow-common:
  &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.0.3}
  entrypoint: ["/usr/bin/dumb-init", "--"]
  user: "${AIRFLOW_UID}"
  depends_on:
    - postgres
    - redis
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  environment:
    &airflow-common-env
    AIRFLOW_UID: "${AIRFLOW_UID}"
    AIRFLOW_GID: "0"
    AIRFLOW_PROD: "false"
    AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    _PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-microsoft-mssql==3.3.0 apache-airflow-providers-mysql==5.0.0 apache-airflow-providers-snowflake==5.0.0 pandas==2.3.1 pyodbc snowflake-connector-python==3.8.0"
    DB_HOST: "postgres"
    REDIS_HOST: "redis"
    AIRFLOW_HOME: "/opt/airflow"
    AIRFLOW_CONN_AIRFLOW_DB: "postgresql://airflow:airflow@postgres:5432/airflow"
    AIRFLOW__WEBSERVER__WORKER_TIMEOUT: "120"
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "300"
  ports:
    - "8080:8080"
  healthcheck:
    test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$(hostname)\" || exit 1"]
    interval: 30s
    timeout: 30s
    retries: 5

services:
  postgres:
    image: postgres:13
    container_name: airflow_pipeline-postgres-1
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - airflow_pipeline_postgres-db-volume:/var/lib/postgresql/data

  redis:
    image: redis:7.2-bookworm
    container_name: airflow_pipeline-redis-1
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-apiserver:
    <<: *airflow-common
    container_name: airflow_pipeline-airflow-apiserver-1
    command: ["airflow", "api", "server"]
    healthcheck:
      test: ["CMD-SHELL", "curl --fail --silent http://localhost:8080/api/v1/version"]
      interval: 30s
      timeout: 30s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow_pipeline-airflow-scheduler-1
    command: ["airflow", "scheduler"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$(hostname)\" || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 5

  airflow-worker:
    <<: *airflow-common
    container_name: airflow_pipeline-airflow-worker-1
    command: ["airflow", "worker"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type WorkerJob --hostname \"$(hostname)\" || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 5

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow_pipeline-airflow-triggerer-1
    command: ["airflow", "triggerer"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type TriggerJob --hostname \"$(hostname)\" || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 5

  airflow-dag-processor:
    <<: *airflow-common
    container_name: airflow_pipeline-airflow-dag-processor-1
    command: ["airflow", "dag-processor"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type DagProcessorJob --hostname \"$(hostname)\" || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 5

  airflow-init:
    <<: *airflow-common
    container_name: airflow_pipeline-airflow-init-1
    command: ["airflow", "db", "init"]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    healthcheck:
      test: ["CMD-SHELL", "airflow db check"]
      interval: 30s
      timeout: 30s
      retries: 5

volumes:
  airflow_pipeline_postgres-db-volume: